\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Implement a Basic Driving Agent}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}\textit  {Observe what you see with the agent's behavior as it takes random actions. Does the \textbf  {smartcab} eventually make it to the destination? Are there any other interesting observations to note?}}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Histogram of Number of Steps taken to destination.\relax }}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Analysis plot of Net reward per trial and the average cumulative reward.\relax }}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Inform the Driving Agent}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\textit  {What states have you identified that are appropriate for modeling the \textbf  {smartcab} and environment? Why do you believe each of these states to be appropriate for this problem?}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\textit  {How many states in total exist for the \textbf  {smartcab} in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?}}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sub1}{{3.1a}{5}}
\newlabel{sub@fig:sub1}{{a}{5}}
\newlabel{fig:sub2}{{3.1b}{5}}
\newlabel{sub@fig:sub2}{{b}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A simulation of 150 trials with Q-Learning algorithm. \relax }}{5}}
\newlabel{fig:test}{{3.1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implement a Q-Learning Driving Agent}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\textit  {What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?}}{5}}
\newlabel{fig:sub1}{{4.1a}{6}}
\newlabel{sub@fig:sub1}{{a}{6}}
\newlabel{fig:sub2}{{4.1b}{6}}
\newlabel{sub@fig:sub2}{{b}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A simulation of 150 trials with Q-Learning algorithm. \relax }}{6}}
\newlabel{fig:test}{{4.1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Improve the Q-Learning Driving Agent}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\textit  {Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?}}{6}}
\newlabel{fig:sub1}{{5.1a}{7}}
\newlabel{sub@fig:sub1}{{a}{7}}
\newlabel{fig:sub2}{{5.1b}{7}}
\newlabel{sub@fig:sub2}{{b}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A simulation of 200 trials with a well trained Q-Learning model. \relax }}{7}}
\newlabel{fig:test}{{5.1}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}CASE: What can a learned/trained agent do?}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\textit  {Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?}}{7}}
\newlabel{fig:sub1}{{5.2a}{8}}
\newlabel{sub@fig:sub1}{{a}{8}}
\newlabel{fig:sub2}{{5.2b}{8}}
\newlabel{sub@fig:sub2}{{b}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces A simulation of 90 training trials. \relax }}{8}}
\newlabel{fig:test}{{5.2}{8}}
\newlabel{fig:sub1}{{5.3a}{9}}
\newlabel{sub@fig:sub1}{{a}{9}}
\newlabel{fig:sub2}{{5.3b}{9}}
\newlabel{sub@fig:sub2}{{b}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces A simulation of 10 evaluation trials using the learned model. \relax }}{9}}
\newlabel{fig:test}{{5.3}{9}}
